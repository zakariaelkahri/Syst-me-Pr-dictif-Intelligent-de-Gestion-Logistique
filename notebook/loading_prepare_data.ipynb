{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69411ae3",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d565208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Type: string (nullable = true)\n",
      " |-- Days for shipping (real): integer (nullable = true)\n",
      " |-- Days for shipment (scheduled): integer (nullable = true)\n",
      " |-- Benefit per order: double (nullable = true)\n",
      " |-- Sales per customer: double (nullable = true)\n",
      " |-- Delivery Status: string (nullable = true)\n",
      " |-- Late_delivery_risk: integer (nullable = true)\n",
      " |-- Category Id: integer (nullable = true)\n",
      " |-- Category Name: string (nullable = true)\n",
      " |-- Customer City: string (nullable = true)\n",
      " |-- Customer Country: string (nullable = true)\n",
      " |-- Customer Email: string (nullable = true)\n",
      " |-- Customer Fname: string (nullable = true)\n",
      " |-- Customer Id: integer (nullable = true)\n",
      " |-- Customer Lname: string (nullable = true)\n",
      " |-- Customer Password: string (nullable = true)\n",
      " |-- Customer Segment: string (nullable = true)\n",
      " |-- Customer State: string (nullable = true)\n",
      " |-- Customer Street: string (nullable = true)\n",
      " |-- Customer Zipcode: integer (nullable = true)\n",
      " |-- Department Id: integer (nullable = true)\n",
      " |-- Department Name: string (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longitude: double (nullable = true)\n",
      " |-- Market: string (nullable = true)\n",
      " |-- Order City: string (nullable = true)\n",
      " |-- Order Country: string (nullable = true)\n",
      " |-- Order Customer Id: integer (nullable = true)\n",
      " |-- order date (DateOrders): string (nullable = true)\n",
      " |-- Order Id: integer (nullable = true)\n",
      " |-- Order Item Cardprod Id: integer (nullable = true)\n",
      " |-- Order Item Discount: double (nullable = true)\n",
      " |-- Order Item Discount Rate: double (nullable = true)\n",
      " |-- Order Item Id: integer (nullable = true)\n",
      " |-- Order Item Product Price: double (nullable = true)\n",
      " |-- Order Item Profit Ratio: double (nullable = true)\n",
      " |-- Order Item Quantity: integer (nullable = true)\n",
      " |-- Sales: double (nullable = true)\n",
      " |-- Order Item Total: double (nullable = true)\n",
      " |-- Order Profit Per Order: double (nullable = true)\n",
      " |-- Order Region: string (nullable = true)\n",
      " |-- Order State: string (nullable = true)\n",
      " |-- Order Status: string (nullable = true)\n",
      " |-- Order Zipcode: integer (nullable = true)\n",
      " |-- Product Card Id: integer (nullable = true)\n",
      " |-- Product Category Id: integer (nullable = true)\n",
      " |-- Product Description: string (nullable = true)\n",
      " |-- Product Image: string (nullable = true)\n",
      " |-- Product Name: string (nullable = true)\n",
      " |-- Product Price: double (nullable = true)\n",
      " |-- Product Status: integer (nullable = true)\n",
      " |-- shipping date (DateOrders): string (nullable = true)\n",
      " |-- Shipping Mode: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparksession import spark\n",
    "\n",
    "df = spark.read.csv('./data/dataset/DataCoSupplyChainDataset.csv',header=True, inferSchema=True)\n",
    "\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e684653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.show(20)\n",
    "\n",
    "# pdf = df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45e618f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = []\n",
    "cat_cols = []\n",
    "\n",
    "# num_pdf = pdf.select_dtypes(['int32', 'float64'])\n",
    "# cat_pdf = pdf.select_dtypes('object')\n",
    "# combined_df = num_pdf.combine(cat_pdf)\n",
    "\n",
    "for col, type in df.dtypes:\n",
    "    if type in ['int', 'double'] and col != 'Late_delivery_risk':\n",
    "        num_cols.append(col)\n",
    "    else: \n",
    "        cat_cols.append(col)\n",
    "\n",
    "# for col in cat_pdf.columns :\n",
    "\n",
    "#         cat_cols.append(col)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cat_indexed = [\n",
    " 'Type(indexed)',\n",
    " 'Delivery Status(indexed)',\n",
    " 'Late_delivery_risk(indexed)',\n",
    " 'Category Name(indexed)',\n",
    " 'Customer City(indexed)',\n",
    " 'Customer Country(indexed)',\n",
    " 'Customer Email(indexed)',\n",
    " 'Customer Fname(indexed)',\n",
    " 'Customer Lname(indexed)',\n",
    " 'Customer Password(indexed)',\n",
    " 'Customer Segment(indexed)',\n",
    " 'Customer State(indexed)',\n",
    " 'Customer Street(indexed)',\n",
    " 'Department Name(indexed)',\n",
    " 'Market(indexed)',\n",
    " 'Order City(indexed)',\n",
    " 'Order Country(indexed)',\n",
    " 'order date (DateOrders)(indexed)',\n",
    " 'Order Region(indexed)',\n",
    " 'Order State(indexed)',\n",
    " 'Order Status(indexed)',\n",
    " 'Product Description(indexed)',\n",
    " 'Product Image(indexed)',\n",
    " 'Product Name(indexed)',\n",
    " 'shipping date (DateOrders)(indexed)',\n",
    " 'Shipping Mode(indexed)']\n",
    "\n",
    "\n",
    "cat_encoded = [\n",
    " 'Type(encoded)',\n",
    " 'Delivery Status(encoded)',\n",
    " 'Late_delivery_risk(encoded)',\n",
    " 'Category Name(encoded)',\n",
    " 'Customer City(encoded)',\n",
    " 'Customer Country(encoded)',\n",
    " 'Customer Email(encoded)',\n",
    " 'Customer Fname(encoded)',\n",
    " 'Customer Lname(encoded)',\n",
    " 'Customer Password(encoded)',\n",
    " 'Customer Segment(encoded)',\n",
    " 'Customer State(encoded)',\n",
    " 'Customer Street(encoded)',\n",
    " 'Department Name(encoded)',\n",
    " 'Market(encoded)',\n",
    " 'Order City(encoded)',\n",
    " 'Order Country(encoded)',\n",
    " 'order date (DateOrders)(encoded)',\n",
    " 'Order Region(encoded)',\n",
    " 'Order State(encoded)',\n",
    " 'Order Status(encoded)',\n",
    " 'Product Description(encoded)',\n",
    " 'Product Image(encoded)',\n",
    " 'Product Name(encoded)',\n",
    " 'shipping date (DateOrders)(encoded)',\n",
    " 'Shipping Mode(encoded)']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57c3cdb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o82.showString.\n: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] User defined function (`StringIndexerModel$$Lambda$6490/0x000001c9a437c698`: (string) => double) failed due to: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.. SQLSTATE: 39000\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:195)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\r\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\r\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\r\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:377)\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:372)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m indexer = StringIndexer(inputCols=cat_cols, outputCols=cat_indexed)\n\u001b[32m      8\u001b[39m df_indexed = indexer.fit(df).transform(df)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[43mdf_indexed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# # encoding with OneHotEncoder\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# encoder = OneHotEncoder(inputCols=cat_indexed, outputCols=cat_encoded)\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# df_encoded = encoder.fit(df_indexed).transform(df_indexed)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \u001b[38;5;66;03m#     outputCol='features'\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ycode\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:285\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m285\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ycode\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:303\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    297\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    298\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    299\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical).\u001b[34m__name__\u001b[39m},\n\u001b[32m    300\u001b[39m     )\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ycode\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ycode\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ycode\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o82.showString.\n: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] User defined function (`StringIndexerModel$$Lambda$6490/0x000001c9a437c698`: (string) => double) failed due to: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.. SQLSTATE: 39000\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:195)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:544)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:497)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:58)\r\n\tat org.apache.spark.sql.classic.Dataset.collectFromPlan(Dataset.scala:2244)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$head$1(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$2(Dataset.scala:2234)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.classic.Dataset.$anonfun$withAction$1(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\r\n\tat org.apache.spark.sql.classic.Dataset.withAction(Dataset.scala:2232)\r\n\tat org.apache.spark.sql.classic.Dataset.head(Dataset.scala:1379)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2810)\r\n\tat org.apache.spark.sql.classic.Dataset.getRows(Dataset.scala:339)\r\n\tat org.apache.spark.sql.classic.Dataset.showString(Dataset.scala:375)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:840)\r\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1(StringIndexer.scala:377)\r\n\tat org.apache.spark.ml.feature.StringIndexerModel.$anonfun$getIndexer$1$adapted(StringIndexer.scala:372)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:402)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:901)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:901)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "\n",
    "# indexing with Stringindexer\n",
    "indexer = StringIndexer(inputCols=cat_cols, outputCols=cat_indexed)\n",
    "df_indexed = indexer.fit(df).transform(df)\n",
    "df_indexed.show()\n",
    "\n",
    "\n",
    "# # encoding with OneHotEncoder\n",
    "# encoder = OneHotEncoder(inputCols=cat_indexed, outputCols=cat_encoded)\n",
    "# df_encoded = encoder.fit(df_indexed).transform(df_indexed)\n",
    "# df_encoded.show()\n",
    "\n",
    "\n",
    "# assembler = VectorAssembler(\n",
    "#     inputCols=[num_cols],\n",
    "#     outputCol='features'\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec758bb8",
   "metadata": {},
   "source": [
    "##### **3. Variables à supprimer :**\n",
    "\n",
    "Colonnes redondantes, personnelles, ou non pertinentes pour la prédiction.\n",
    "\n",
    "\n",
    "| Colonne                                                                                                            | Raison                                                                                           |\n",
    "| ------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------ |\n",
    "| `Customer Fname`, `Customer Lname`, `Customer Email`, `Customer Password`                                          | Données personnelles (Attention à RGPD).                                                                  |\n",
    "| `Customer City`, `Customer State`, `Customer Street`, `Customer Zipcode`                                           | Trop détaillées, redondantes avec `Order` location.                                              |\n",
    "| `Order Id`, `Customer Id`, `Order Item Id`, `Order Customer Id`, `Order Item Cardprod Id`                          | Identifiants sans valeur prédictive.                                                             |\n",
    "| `Product Card Id`, `Product Category Id`, `Category Id`, `Department Id`                                           | IDs sans signification directe.                                                                  |\n",
    "| `Product Description`, `Product Image`, `Product Name`                                                             | Texte libre ou image, inutile pour ce modèle.                                                    |\n",
    "| `Order Item Total`, `Order Profit Per Order`, `Benefit per order`, `Sales per customer`, `Order Item Profit Ratio` | Variables dérivées de `Sales`, créent du *data leakage* si elles sont calculées après livraison. |\n",
    "| `Order Zipcode`, `Customer Country` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c575e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "olumns_redondantes  = [\n",
    "    'Type',\n",
    "    'Days for shipping (real)',\n",
    "    'Delivery Status',\n",
    "    'Customer Fname',\n",
    "    'Customer Lname',\n",
    "    'Customer Email',\n",
    "    'Customer Password',\n",
    "    'Order Id',\n",
    "    'Customer Id',\n",
    "    'Order Item Id',\n",
    "    'Order Customer Id',\n",
    "    'Order Item Cardprod Id',\n",
    "    'Product Card Id',\n",
    "    'Product Category Id',\n",
    "    'Department Id',\n",
    "    'Product Description',\n",
    "    'Product Image',\n",
    "    'Product Name'  ,\n",
    "    'Order Item Total',\n",
    "    'Order Profit Per Order',\n",
    "    'Benefit per order',\n",
    "    'Sales per customer',\n",
    "    'Order Item Profit Ratio',\n",
    "    'Customer Country',\n",
    "    'rder Zipcode',\n",
    "    'Product Status',\n",
    "    'Customer State',\n",
    "    'Customer Street',\n",
    "    'Customer Zipcode',\n",
    "    'Department Name',\n",
    "    'Latitude',\n",
    "    'Longitude',\n",
    "    'Market',\n",
    "    'Order City',\n",
    "    'Order Country',\n",
    "    'Order Item Discount',\n",
    "    'Order Item Discount Rate',\n",
    "    'Order Item Product Price',\n",
    "    'Order Item Quantity',\n",
    "    'Sales',\n",
    "    'Order Status',\n",
    "    'Product Card Id',\n",
    "    'Product Price'\n",
    "    'Product Status',\n",
    "    'Shipping date (DateOrders)',\n",
    "    'Shipping Mode',\n",
    "    'Category Name',\n",
    "    'Customer City',\n",
    "    'Order Zipcode',\n",
    "    'Order State',\n",
    "    'Product Price',\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "df_columns_cleaned =  df.drop(*olumns_redondantes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64e30912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------+-----------------------------+-----------------+------------------+---------------+------------------+-----------+-------------+-------------+----------------+--------------+--------------+-----------+--------------+-----------------+----------------+--------------+---------------+----------------+-------------+---------------+--------+---------+------+----------+-------------+-----------------+-----------------------+--------+----------------------+-------------------+------------------------+-------------+------------------------+-----------------------+-------------------+-----+----------------+----------------------+------------+-----------+------------+-------------+---------------+-------------------+-------------------+-------------+------------+-------------+--------------+--------------------------+-------------+\n",
      "|Type|Days for shipping (real)|Days for shipment (scheduled)|Benefit per order|Sales per customer|Delivery Status|Late_delivery_risk|Category Id|Category Name|Customer City|Customer Country|Customer Email|Customer Fname|Customer Id|Customer Lname|Customer Password|Customer Segment|Customer State|Customer Street|Customer Zipcode|Department Id|Department Name|Latitude|Longitude|Market|Order City|Order Country|Order Customer Id|order date (DateOrders)|Order Id|Order Item Cardprod Id|Order Item Discount|Order Item Discount Rate|Order Item Id|Order Item Product Price|Order Item Profit Ratio|Order Item Quantity|Sales|Order Item Total|Order Profit Per Order|Order Region|Order State|Order Status|Order Zipcode|Product Card Id|Product Category Id|Product Description|Product Image|Product Name|Product Price|Product Status|shipping date (DateOrders)|Shipping Mode|\n",
      "+----+------------------------+-----------------------------+-----------------+------------------+---------------+------------------+-----------+-------------+-------------+----------------+--------------+--------------+-----------+--------------+-----------------+----------------+--------------+---------------+----------------+-------------+---------------+--------+---------+------+----------+-------------+-----------------+-----------------------+--------+----------------------+-------------------+------------------------+-------------+------------------------+-----------------------+-------------------+-----+----------------+----------------------+------------+-----------+------------+-------------+---------------+-------------------+-------------------+-------------+------------+-------------+--------------+--------------------------+-------------+\n",
      "+----+------------------------+-----------------------------+-----------------+------------------+---------------+------------------+-----------+-------------+-------------+----------------+--------------+--------------+-----------+--------------+-----------------+----------------+--------------+---------------+----------------+-------------+---------------+--------+---------+------+----------+-------------+-----------------+-----------------------+--------+----------------------+-------------------+------------------------+-------------+------------------------+-----------------------+-------------------+-----+----------------+----------------------+------------+-----------+------------+-------------+---------------+-------------------+-------------------+-------------+------------+-------------+--------------+--------------------------+-------------+\n",
      "\n",
      "+-------------------+\n",
      "|Product Category Id|\n",
      "+-------------------+\n",
      "|                 17|\n",
      "|                 29|\n",
      "|                 24|\n",
      "|                 29|\n",
      "|                 24|\n",
      "|                 24|\n",
      "|                 24|\n",
      "|                 13|\n",
      "|                 12|\n",
      "|                 17|\n",
      "|                 17|\n",
      "|                  9|\n",
      "|                 29|\n",
      "|                 41|\n",
      "|                 37|\n",
      "|                  9|\n",
      "|                  9|\n",
      "|                  9|\n",
      "|                 17|\n",
      "|                 17|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "df.filter(f.col('Product Status') != 0).show()\n",
    "\n",
    "df.select('Product Category Id').filter(f.col('Product Category Id') != 73).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7690274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+------------------+-----------+----------------+--------------+-----------+\n",
      "|Days for shipment (scheduled)|Late_delivery_risk|Category Id|Customer Segment|  Order Region|order_month|\n",
      "+-----------------------------+------------------+-----------+----------------+--------------+-----------+\n",
      "|                            4|                 0|         73|        Consumer|Southeast Asia|          1|\n",
      "|                            4|                 1|         73|        Consumer|    South Asia|          1|\n",
      "|                            4|                 0|         73|        Consumer|    South Asia|          1|\n",
      "|                            4|                 0|         73|     Home Office|       Oceania|          1|\n",
      "|                            4|                 0|         73|       Corporate|       Oceania|          1|\n",
      "|                            4|                 0|         73|        Consumer|       Oceania|          1|\n",
      "|                            1|                 1|         73|     Home Office|  Eastern Asia|          1|\n",
      "|                            1|                 1|         73|       Corporate|  Eastern Asia|          1|\n",
      "|                            2|                 1|         73|       Corporate|  Eastern Asia|          1|\n",
      "|                            1|                 1|         73|       Corporate|  Eastern Asia|          1|\n",
      "|                            2|                 0|         73|       Corporate|  Eastern Asia|          1|\n",
      "|                            2|                 1|         73|        Consumer|Southeast Asia|          1|\n",
      "|                            2|                 1|         73|       Corporate|Southeast Asia|          1|\n",
      "|                            1|                 1|         73|       Corporate|    South Asia|          1|\n",
      "|                            1|                 1|         73|       Corporate|    South Asia|          1|\n",
      "|                            1|                 1|         73|       Corporate|    South Asia|          1|\n",
      "|                            2|                 1|         73|       Corporate|  Eastern Asia|          1|\n",
      "|                            1|                 1|         73|       Corporate|    South Asia|          1|\n",
      "|                            1|                 1|         73|        Consumer|    South Asia|          1|\n",
      "|                            0|                 0|         73|        Consumer|    South Asia|          1|\n",
      "+-----------------------------+------------------+-----------+----------------+--------------+-----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_columns_cleaned = df_columns_cleaned.withColumn(\n",
    "    'order_date',\n",
    "    f.to_timestamp('order date (DateOrders)', \"M/d/yyyy H:mm\")\n",
    ")\n",
    "\n",
    "df_columns_cleaned = df_columns_cleaned.withColumn('order_month', f.month('order_date'))\n",
    "\n",
    "df_columns_cleaned = df_columns_cleaned.drop('order date (DateOrders)', 'order_date')\n",
    "\n",
    "df_columns_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b0546b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columns_cleaned.coalesce(1).write.csv(\n",
    "    \"./data/data_cleaned\",\n",
    "    header=True,\n",
    "    mode=\"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84059d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|order_month|count(order_month)|\n",
      "+-----------+------------------+\n",
      "|         12|             12764|\n",
      "|          1|             17979|\n",
      "|          6|             15139|\n",
      "|          3|             15919|\n",
      "|          5|             15976|\n",
      "|          9|             15489|\n",
      "|          4|             15435|\n",
      "|          8|             15912|\n",
      "|          7|             15922|\n",
      "|         10|             12955|\n",
      "|         11|             12500|\n",
      "|          2|             14529|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_columns_cleaned.groupBy('order_month').agg(\n",
    "    f.count('order_month'),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5687f7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
